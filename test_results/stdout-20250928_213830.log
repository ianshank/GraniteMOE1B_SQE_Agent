============================= test session starts ==============================
platform darwin -- Python 3.13.5, pytest-8.3.2, pluggy-1.5.0 -- /opt/anaconda3/bin/python
cachedir: .pytest_cache
benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /Users/iancruickshank/GraniteMOE1B_SQE_Agent
configfile: pytest.ini
plugins: anyio-4.11.0, cov-5.0.0, asyncio-1.1.0, dash-3.2.0, Faker-37.6.0, benchmark-5.1.0, langsmith-0.4.21, mock-3.14.0
asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... 
----------------------------- live log collection ------------------------------
INFO     numexpr.utils:utils.py:161 NumExpr defaulting to 10 threads.
INFO     test_metrics:test_metrics.py:33 scikit-learn available for reference metric calculation
INFO     test_evaluation_helpers:test_evaluation_helpers.py:32 PyTorch available for testing
collected 85 items

tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[True-True] PASSED [  1%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[False-False] PASSED [  2%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[1-True0] PASSED [  3%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[0-False0] PASSED [  4%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[true-True] PASSED [  5%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[false-False] PASSED [  7%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[yes-True] PASSED [  8%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[no-False] PASSED [  9%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[1-True1] PASSED [ 10%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[0-False1] PASSED [ 11%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[t-True] PASSED [ 12%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[f-False] PASSED [ 14%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[y-True] PASSED [ 15%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[n-False] PASSED [ 16%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[on-True] PASSED [ 17%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[off-False] PASSED [ 18%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[TRUE-True] PASSED [ 20%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[FALSE-False] PASSED [ 21%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[   yes   -True] PASSED [ 22%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[   no   -False] PASSED [ 23%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[None-None] PASSED [ 24%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[invalid-None] PASSED [ 25%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[value22-None] PASSED [ 27%]
tests/unit/test_telemetry_config.py::TestBooleanNormalization::test_normalize_bool[value23-None] PASSED [ 28%]
tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[tag1,tag2,tag3-expected0] PASSED [ 29%]
tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[tag1, tag2, tag3-expected1] PASSED [ 30%]
tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[-expected2] PASSED [ 31%]
tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[None-expected3] PASSED [ 32%]
tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[value4-expected4] PASSED [ 34%]
tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[value5-expected5] PASSED [ 35%]
tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[value6-expected6] PASSED [ 36%]
tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[123-expected7] FAILED [ 37%]
tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[value8-expected8] PASSED [ 38%]
tests/unit/test_telemetry_config.py::TestTelemetryConfig::test_default_config PASSED [ 40%]
tests/unit/test_telemetry_config.py::TestTelemetryConfig::test_tb_log_dir_validation PASSED [ 41%]
tests/unit/test_telemetry_config.py::TestTelemetryConfig::test_tags_normalization PASSED [ 42%]
tests/unit/test_telemetry_config.py::TestTelemetryConfig::test_merged_with PASSED [ 43%]
tests/unit/test_telemetry_config.py::TestLoadTelemetryFromSources::test_load_from_env PASSED [ 44%]
tests/unit/test_telemetry_config.py::TestLoadTelemetryFromSources::test_load_from_cli_args PASSED [ 45%]
tests/unit/test_telemetry_config.py::TestLoadTelemetryFromSources::test_cli_overrides_env PASSED [ 47%]
tests/unit/test_telemetry_config.py::TestLoadTelemetryFromSources::test_invalid_log_interval_steps 
-------------------------------- live log call ---------------------------------
WARNING  src.config.telemetry:telemetry.py:140 Invalid log interval override invalid
FAILED                                                                   [ 48%]
tests/unit/test_telemetry_config.py::TestLoadTelemetryFromSources::test_object_with_attributes PASSED [ 49%]
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_file_system_artifacts 
-------------------------------- live log call ---------------------------------
INFO     src.telemetry.experiment:experiment.py:222 TensorBoard writer initialised at /private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_file_system_artifacts0/runs
FAILED                                                                   [ 50%]
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_metrics_with_different_types 
-------------------------------- live log call ---------------------------------
INFO     src.telemetry.experiment:experiment.py:222 TensorBoard writer initialised at /private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_metrics_with_different_ty0/runs
PASSED                                                                   [ 51%]
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_run_name_generation FAILED [ 52%]
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_git_sha_detection PASSED [ 54%]
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_number_validation PASSED [ 55%]
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_artifact_with_nonexistent_path 
-------------------------------- live log call ---------------------------------
WARNING  src.telemetry.experiment:experiment.py:110 Artifact path /path/does/not/exist.txt does not exist; skipping upload
PASSED                                                                   [ 56%]
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_graceful_multiple_finish PASSED [ 57%]
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_no_metrics PASSED [ 58%]
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_experiment_logger_with_real_directory_structure FAILED [ 60%]
tests/unit/test_metrics.py::TestClassificationMetrics::test_binary_classification FAILED [ 61%]
tests/unit/test_metrics.py::TestClassificationMetrics::test_multiclass_classification FAILED [ 62%]
tests/unit/test_metrics.py::TestClassificationMetrics::test_empty_predictions PASSED [ 63%]
tests/unit/test_metrics.py::TestClassificationMetrics::test_mismatched_lengths PASSED [ 64%]
tests/unit/test_metrics.py::TestClassificationMetrics::test_probabilities_without_sklearn PASSED [ 65%]
tests/unit/test_metrics.py::TestRegressionMetrics::test_basic_regression FAILED [ 67%]
tests/unit/test_metrics.py::TestRegressionMetrics::test_perfect_predictions FAILED [ 68%]
tests/unit/test_metrics.py::TestRegressionMetrics::test_constant_predictions PASSED [ 69%]
tests/unit/test_metrics.py::TestRegressionMetrics::test_empty_predictions_regression FAILED [ 70%]
tests/unit/test_metrics.py::TestRegressionMetrics::test_mismatched_lengths_regression PASSED [ 71%]
tests/unit/test_metrics.py::TestTextGenerationMetrics::test_basic_text_metrics 
-------------------------------- live log call ---------------------------------
INFO     absl:rouge_scorer.py:83 Using default tokenizer.
FAILED                                                                   [ 72%]
tests/unit/test_metrics.py::TestTextGenerationMetrics::test_multiple_references 
-------------------------------- live log call ---------------------------------
INFO     absl:rouge_scorer.py:83 Using default tokenizer.
WARNING  src.eval.metrics:metrics.py:128 Failed to compute advanced text metrics: ChrF, as implemented by sacrebleu, requires the same number of references for each prediction
PASSED                                                                   [ 74%]
tests/unit/test_metrics.py::TestTextGenerationMetrics::test_empty_text_predictions 
-------------------------------- live log call ---------------------------------
WARNING  src.eval.metrics:metrics.py:128 Failed to compute advanced text metrics: list index out of range
FAILED                                                                   [ 75%]
tests/unit/test_metrics.py::TestTextGenerationMetrics::test_mismatched_lengths_text 
-------------------------------- live log call ---------------------------------
WARNING  src.eval.metrics:metrics.py:128 Failed to compute advanced text metrics: Mismatch in the number of predictions (2) and references (1)
FAILED                                                                   [ 76%]
tests/unit/test_metrics.py::TestTextGenerationMetrics::test_invalid_references_format 
-------------------------------- live log call ---------------------------------
INFO     absl:rouge_scorer.py:83 Using default tokenizer.
FAILED                                                                   [ 77%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_split_batch_tuple PASSED [ 78%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_split_batch_dict PASSED [ 80%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_split_batch_dict_alternative_keys PASSED [ 81%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_split_batch_error_missing_keys PASSED [ 82%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_split_batch_error_wrong_type PASSED [ 83%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_forward_callable_model PASSED [ 84%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_forward_model_with_eval_step PASSED [ 85%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_forward_error_invalid_model PASSED [ 87%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_flatten_simple_values PASSED [ 88%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_flatten_lists PASSED [ 89%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_flatten_torch_tensor PASSED [ 90%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_flatten_probabilities PASSED [ 91%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_flatten_probabilities_torch FAILED [ 92%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_percentile PASSED [ 94%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_is_number PASSED [ 95%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_evaluate_classification 
-------------------------------- live log call ---------------------------------
INFO     src.eval.evaluate:evaluate.py:104 Saved evaluation report to /private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_evaluate_classification0/eval_results/eval_report.json
FAILED                                                                   [ 96%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_evaluate_regression FAILED [ 97%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_evaluate_text 
-------------------------------- live log call ---------------------------------
INFO     absl:rouge_scorer.py:83 Using default tokenizer.
INFO     src.eval.evaluate:evaluate.py:104 Saved evaluation report to /private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_evaluate_text0/text_results/eval_report.json
FAILED                                                                   [ 98%]
tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_evaluate_error_invalid_task PASSED [100%]

=================================== FAILURES ===================================
_______________ TestTagSplitting.test_split_tags[123-expected7] ________________

self = <test_telemetry_config.TestTagSplitting object at 0x10c282350>
value = 123, expected = ['123']

    @pytest.mark.parametrize(
        "value,expected",
        [
            ("tag1,tag2,tag3", ["tag1", "tag2", "tag3"]),
            ("tag1, tag2, tag3", ["tag1", "tag2", "tag3"]),
            ("", []),
            (None, []),
            (["tag1", "tag2"], ["tag1", "tag2"]),
            (["tag1,tag2", "tag3"], ["tag1", "tag2", "tag3"]),
            (["", "tag1", "", "tag2"], ["tag1", "tag2"]),
            (123, ["123"]),  # Non-string/list values are converted to string
            (["tag1", 123], ["tag1", "123"]),
        ],
    )
    def test_split_tags(self, value: Any, expected: list[str]) -> None:
        """Verify tag splitting handles various input formats correctly."""
        result = _split_tags(value)
        logger.debug("_split_tags(%r) = %r (expected: %r)", value, result, expected)
>       assert result == expected
E       AssertionError: assert [] == ['123']
E         
E         Right contains one more item: '123'
E         
E         Full diff:
E         + []
E         - [
E         -     '123',
E         - ]

tests/unit/test_telemetry_config.py:93: AssertionError
_________ TestLoadTelemetryFromSources.test_invalid_log_interval_steps _________

self = <test_telemetry_config.TestLoadTelemetryFromSources object at 0x10c277360>

    def test_invalid_log_interval_steps(self) -> None:
        """Test handling of invalid log interval steps."""
        # Test with invalid string
        cli_args = {"log_interval_steps": "invalid"}
        config = load_telemetry_from_sources(cli_args=cli_args)
        assert config.log_interval_steps == 50  # Default value
    
        # Test with negative value
        cli_args = {"log_interval_steps": -10}
>       config = load_telemetry_from_sources(cli_args=cli_args)

tests/unit/test_telemetry_config.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
granite-test-generator/src/config/telemetry.py:143: in load_telemetry_from_sources
    return base_cfg.merged_with(**overrides)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = TelemetryConfig(enable_wandb=False, wandb_project='telemetry-tests', wandb_entity=None, wandb_run_name=None, wandb_tags=[], enable_tensorboard=False, tb_log_dir='test_runs', log_interval_steps=50)
overrides = {'log_interval_steps': -10}
data = {'enable_tensorboard': False, 'enable_wandb': False, 'log_interval_steps': -10, 'tb_log_dir': 'test_runs', ...}

    def merged_with(self, **overrides: Any) -> "TelemetryConfig":
        data = self.dict()
        data.update({k: v for k, v in overrides.items() if v is not None})
>       return TelemetryConfig(**data)
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for TelemetryConfig
E       log_interval_steps
E         Input should be greater than or equal to 1 [type=greater_than_equal, input_value=-10, input_type=int]
E           For further information visit https://errors.pydantic.dev/2.8/v/greater_than_equal

granite-test-generator/src/config/telemetry.py:78: ValidationError
------------------------------ Captured log call -------------------------------
WARNING  src.config.telemetry:telemetry.py:140 Invalid log interval override invalid
_______________ TestExperimentLogger.test_file_system_artifacts ________________

self = <test_experiment_logger.TestExperimentLogger object at 0x10cd25090>
tmp_path = PosixPath('/private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_file_system_artifacts0')

    def test_file_system_artifacts(self, tmp_path: Path) -> None:
        """Test handling of file system artifacts."""
        # Create test files
        artifact_dir = tmp_path / "artifacts"
        artifact_dir.mkdir()
        test_file = artifact_dir / "test_file.txt"
        test_file.write_text("This is a test file")
    
        # Create JSON file
        json_file = artifact_dir / "metrics.json"
        json_data = {"accuracy": 0.92, "f1": 0.88}
        with open(json_file, "w") as f:
            json.dump(json_data, f)
    
        # Configure logger with TensorBoard enabled
        config = TelemetryConfig(
            enable_tensorboard=True,
            tb_log_dir=str(tmp_path / "runs"),
        )
    
        # Use real filesystem interactions
        with ExperimentLogger(config, {"model": {"type": "test"}}) as logger:
            # Log artifacts
            logger.log_artifact(test_file)
            logger.log_artifact(json_file, name="results", type="metrics")
    
            # Log metrics and parameters
            logger.log_metrics(1, accuracy=0.92, f1=0.88)
            logger.log_params(learning_rate=0.01, batch_size=32)
            logger.set_summary(final_accuracy=0.95)
    
        # Verify TensorBoard output directory was created
        tb_dir = tmp_path / "runs"
        assert tb_dir.exists()
        assert any(tb_dir.glob("*"))  # Should contain at least one file
>       logger.debug(f"TensorBoard directory contents: {list(tb_dir.glob('*'))}")
E       AttributeError: 'ExperimentLogger' object has no attribute 'debug'

tests/unit/test_experiment_logger.py:79: AttributeError
------------------------------ Captured log call -------------------------------
INFO     src.telemetry.experiment:experiment.py:222 TensorBoard writer initialised at /private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_file_system_artifacts0/runs
________________ TestExperimentLogger.test_run_name_generation _________________

self = <test_experiment_logger.TestExperimentLogger object at 0x10c277950>

    def test_run_name_generation(self) -> None:
        """Test automatic run name generation."""
        # Test with minimal config
        config = TelemetryConfig()
        logger1 = ExperimentLogger(config, {"model": {"type": "test"}})
        name1 = logger1._run_name
>       assert name1.startswith("test-dataset-")
E       AttributeError: 'NoneType' object has no attribute 'startswith'

tests/unit/test_experiment_logger.py:131: AttributeError
__ TestExperimentLogger.test_experiment_logger_with_real_directory_structure ___

self = <test_experiment_logger.TestExperimentLogger object at 0x10c993850>
tmp_path = PosixPath('/private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_experiment_logger_with_re0')

    def test_experiment_logger_with_real_directory_structure(self, tmp_path: Path) -> None:
        """Test ExperimentLogger with a real directory structure."""
        # Create directory structure
        output_dir = tmp_path / "experiment"
        checkpoints_dir = output_dir / "checkpoints"
        checkpoints_dir.mkdir(parents=True)
    
        # Create a checkpoint file
        checkpoint_file = checkpoints_dir / "model.pt"
        checkpoint_file.write_text("dummy checkpoint")
    
        # Create an evaluation report
        eval_dir = output_dir / "eval"
        eval_dir.mkdir()
        eval_report = eval_dir / "eval_report.json"
        eval_data = {"accuracy": 0.95, "f1": 0.93}
        with open(eval_report, "w") as f:
            json.dump(eval_data, f)
    
        # Configure telemetry
        config = TelemetryConfig(
            enable_tensorboard=True,
            tb_log_dir=str(tmp_path / "tb_logs"),
        )
    
        # Create a complete experiment workflow
>       with ExperimentLogger(config, {"model": "test"}) as logger:

tests/unit/test_experiment_logger.py:262: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
granite-test-generator/src/telemetry/experiment.py:38: in __enter__
    self.start_run()
granite-test-generator/src/telemetry/experiment.py:48: in start_run
    self._run_name = self._run_name or self._derive_run_name()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.telemetry.experiment.ExperimentLogger object at 0x16e039d10>

    def _derive_run_name(self) -> str:
>       model_name = self._config_snapshot.get("model", {}).get("type") or self._config_snapshot.get("model_name", "model")
E       AttributeError: 'str' object has no attribute 'get'

granite-test-generator/src/telemetry/experiment.py:228: AttributeError
_____________ TestClassificationMetrics.test_binary_classification _____________

self = <test_metrics.TestClassificationMetrics object at 0x17ca6c550>

    def test_binary_classification(self) -> None:
        """Test metrics for binary classification."""
        preds = [1, 0, 1, 1, 0, 1, 0, 0]
        targets = [1, 0, 1, 0, 0, 1, 1, 0]
        probabilities = [0.9, 0.2, 0.8, 0.7, 0.3, 0.6, 0.4, 0.1]
    
        metrics = compute_classification_metrics(preds, targets, probabilities)
        logger.debug(f"Binary classification metrics: {metrics}")
    
        # Check basic metrics
        assert "accuracy" in metrics
        assert math.isclose(metrics["accuracy"], 0.75)  # 6 correct out of 8
    
        # Check precision, recall, F1
        assert "precision_macro" in metrics
        assert "recall_macro" in metrics
        assert "f1_macro" in metrics
        assert "precision_micro" in metrics
        assert "recall_micro" in metrics
        assert "f1_micro" in metrics
>       assert "f1_weighted" in metrics
E       AssertionError: assert 'f1_weighted' in {'accuracy': 0.75, 'auroc': 0.875, 'f1_macro': 0.75, 'f1_micro': 0.75, ...}

tests/unit/test_metrics.py:68: AssertionError
___________ TestClassificationMetrics.test_multiclass_classification ___________

self = <test_metrics.TestClassificationMetrics object at 0x3013220d0>

    def test_multiclass_classification(self) -> None:
        """Test metrics for multiclass classification."""
        preds = [0, 1, 2, 0, 1, 2, 0, 1, 2]
        targets = [0, 1, 2, 1, 1, 2, 2, 1, 0]
    
        # One-hot encoded probabilities for multiclass
        probs = [
            [0.8, 0.1, 0.1],  # Pred 0 with 0.8 confidence
            [0.1, 0.7, 0.2],  # Pred 1 with 0.7 confidence
            [0.0, 0.3, 0.7],  # Pred 2 with 0.7 confidence
            [0.6, 0.3, 0.1],  # Pred 0 with 0.6 confidence
            [0.2, 0.6, 0.2],  # Pred 1 with 0.6 confidence
            [0.1, 0.2, 0.7],  # Pred 2 with 0.7 confidence
            [0.5, 0.3, 0.2],  # Pred 0 with 0.5 confidence
            [0.3, 0.5, 0.2],  # Pred 1 with 0.5 confidence
            [0.1, 0.1, 0.8],  # Pred 2 with 0.8 confidence
        ]
    
        metrics = compute_classification_metrics(preds, targets, probs)
        logger.debug(f"Multiclass classification metrics: {metrics}")
    
        # Check basic metrics
        assert "accuracy" in metrics
>       assert math.isclose(metrics["accuracy"], 5/9)  # 5 correct out of 9
E       assert False
E        +  where False = <built-in function isclose>(0.6666666666666666, (5 / 9))
E        +    where <built-in function isclose> = math.isclose

tests/unit/test_metrics.py:104: AssertionError
_________________ TestRegressionMetrics.test_basic_regression __________________

self = <test_metrics.TestRegressionMetrics object at 0x10cd251d0>

    def test_basic_regression(self) -> None:
        """Test basic regression metrics."""
        preds = [1.0, 2.0, 3.0, 4.0, 5.0]
        targets = [1.1, 1.9, 3.2, 4.1, 4.8]
    
        metrics = compute_regression_metrics(preds, targets)
        logger.debug(f"Basic regression metrics: {metrics}")
    
        # Check metrics
        assert "mae" in metrics  # Mean Absolute Error
>       assert "mse" in metrics  # Mean Squared Error
E       AssertionError: assert 'mse' in {'mae': 0.14000000000000004, 'r2': 0.9881822088525999, 'rmse': 0.14832396974191334}

tests/unit/test_metrics.py:179: AssertionError
________________ TestRegressionMetrics.test_perfect_predictions ________________

self = <test_metrics.TestRegressionMetrics object at 0x3083e9e50>

    def test_perfect_predictions(self) -> None:
        """Test metrics with perfect predictions."""
        preds = [1.0, 2.0, 3.0, 4.0, 5.0]
        targets = [1.0, 2.0, 3.0, 4.0, 5.0]
    
        metrics = compute_regression_metrics(preds, targets)
        logger.debug(f"Perfect regression metrics: {metrics}")
    
        assert metrics["mae"] == 0.0
>       assert metrics["mse"] == 0.0
E       KeyError: 'mse'

tests/unit/test_metrics.py:211: KeyError
___________ TestRegressionMetrics.test_empty_predictions_regression ____________

self = <test_metrics.TestRegressionMetrics object at 0x30229dba0>

    def test_empty_predictions_regression(self) -> None:
        """Test handling of empty predictions."""
        preds = []
        targets = []
    
        metrics = compute_regression_metrics(preds, targets)
        logger.debug(f"Empty regression metrics: {metrics}")
    
        # Should have default values
        assert metrics["mae"] == 0.0
>       assert metrics["mse"] == 0.0
E       KeyError: 'mse'

tests/unit/test_metrics.py:236: KeyError
______________ TestTextGenerationMetrics.test_basic_text_metrics _______________

self = <test_metrics.TestTextGenerationMetrics object at 0x3083e9f90>

    def test_basic_text_metrics(self) -> None:
        """Test basic text generation metrics."""
        preds = [
            "The cat sat on the mat.",
            "I like to eat pizza.",
            "Machine learning is fun."
        ]
    
        refs = [
            ["The cat sat on the mat."],  # Exact match
            ["I enjoy eating pizza."],     # Similar but not exact
            ["Natural language processing is interesting."]  # Different
        ]
    
        latencies = [100.0, 150.0, 200.0]  # ms
    
        metrics = compute_text_generation_metrics(preds, refs, latencies_ms=latencies)
        logger.debug(f"Text generation metrics: {metrics}")
    
        # Check metrics
        assert "exact_match" in metrics
        assert metrics["exact_match"] == 1/3  # One exact match out of three
    
        # Latency metrics
        assert "latency_ms_avg" in metrics
        assert math.isclose(metrics["latency_ms_avg"], 150.0)
>       assert "latency_ms_p50" in metrics
E       AssertionError: assert 'latency_ms_p50' in {'bleu': 0.4899259467249772, 'chrf': 39.11440468239306, 'exact_match': 0.3333333333333333, 'latency_ms_avg': 150.0, ...}

tests/unit/test_metrics.py:278: AssertionError
------------------------------ Captured log call -------------------------------
INFO     absl:rouge_scorer.py:83 Using default tokenizer.
____________ TestTextGenerationMetrics.test_empty_text_predictions _____________

self = <test_metrics.TestTextGenerationMetrics object at 0x30229de00>

    def test_empty_text_predictions(self) -> None:
        """Test handling of empty text predictions."""
        preds = []
        refs = []
    
        metrics = compute_text_generation_metrics(preds, refs)
        logger.debug(f"Empty text metrics: {metrics}")
    
        # Should have default values
        assert metrics["exact_match"] == 0.0
>       assert metrics["bleu"] == 0.0
E       KeyError: 'bleu'

tests/unit/test_metrics.py:314: KeyError
------------------------------ Captured log call -------------------------------
WARNING  src.eval.metrics:metrics.py:128 Failed to compute advanced text metrics: list index out of range
____________ TestTextGenerationMetrics.test_mismatched_lengths_text ____________

self = <test_metrics.TestTextGenerationMetrics object at 0x30229df30>

    def test_mismatched_lengths_text(self) -> None:
        """Test handling of mismatched prediction and reference lengths."""
        preds = ["Text one", "Text two"]
        refs = [["Reference one"]]
    
>       with pytest.raises(ValueError, match="Predictions and references must share the same length"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/unit/test_metrics.py:321: Failed
------------------------------ Captured log call -------------------------------
WARNING  src.eval.metrics:metrics.py:128 Failed to compute advanced text metrics: Mismatch in the number of predictions (2) and references (1)
___________ TestTextGenerationMetrics.test_invalid_references_format ___________

self = <test_metrics.TestTextGenerationMetrics object at 0x308644dd0>

    def test_invalid_references_format(self) -> None:
        """Test handling of invalid references format."""
        preds = ["Text one", "Text two"]
        # References should be a list of lists, not a list of strings
        refs = ["Reference one", "Reference two"]
    
>       with pytest.raises(ValueError, match="Each reference must be a list of strings"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/unit/test_metrics.py:330: Failed
------------------------------ Captured log call -------------------------------
INFO     absl:rouge_scorer.py:83 Using default tokenizer.
____________ TestEvaluationHelpers.test_flatten_probabilities_torch ____________

self = <test_evaluation_helpers.TestEvaluationHelpers object at 0x308641c50>

    @pytest.mark.skipif(not HAS_TORCH, reason="PyTorch not available")
    def test_flatten_probabilities_torch(self) -> None:
        """Test flattening probability tensors."""
        # Tensor with exact match length
        tensor = torch.tensor([0.1, 0.2, 0.3])
>       assert _flatten_probabilities(tensor, 3) == [0.1, 0.2, 0.3]
E       AssertionError: assert [0.1000000014...0001192092896] == [0.1, 0.2, 0.3]
E         
E         At index 0 diff: 0.10000000149011612 != 0.1
E         
E         Full diff:
E           [
E         -     0.1,
E         -     0.2,...
E         
E         ...Full output truncated (5 lines hidden), use '-vv' to show

tests/unit/test_evaluation_helpers.py:147: AssertionError
______________ TestEvaluationHelpers.test_evaluate_classification ______________

self = <test_evaluation_helpers.TestEvaluationHelpers object at 0x3083a5cd0>
tmp_path = PosixPath('/private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_evaluate_classification0')

    def test_evaluate_classification(self, tmp_path: Path) -> None:
        """Test evaluate function with classification task."""
        # Simple model that always predicts class 1
        model = lambda x: 1
    
        # Simple dataloader with 5 samples
        class SimpleDataLoader:
            def __init__(self):
                self.data = [
                    ([0], 1),  # Correct
                    ([0], 0),  # Incorrect
                    ([0], 1),  # Correct
                    ([0], 1),  # Correct
                    ([0], 0),  # Incorrect
                ]
    
            def __iter__(self):
                return iter(self.data)
    
        dataloader = SimpleDataLoader()
        output_dir = tmp_path / "eval_results"
    
        # Create experiment logger
        config = TelemetryConfig()
        logger = ExperimentLogger(config, {"model": "test_model"})
    
        # Run evaluation
        metrics = evaluate(
            model,
            dataloader,
            task_type="classification",
            experiment_logger=logger,
            output_dir=output_dir,
            epoch=1,
        )
    
        # Check metrics
>       logger.debug(f"Classification metrics: {metrics}")
E       AttributeError: 'ExperimentLogger' object has no attribute 'debug'

tests/unit/test_evaluation_helpers.py:216: AttributeError
------------------------------ Captured log call -------------------------------
INFO     src.eval.evaluate:evaluate.py:104 Saved evaluation report to /private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_evaluate_classification0/eval_results/eval_report.json
________________ TestEvaluationHelpers.test_evaluate_regression ________________

self = <test_evaluation_helpers.TestEvaluationHelpers object at 0x3083bb280>
tmp_path = PosixPath('/private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_evaluate_regression0')

    def test_evaluate_regression(self, tmp_path: Path) -> None:
        """Test evaluate function with regression task."""
        # Simple model that always predicts y = 2x
        model = lambda x: x * 2
    
        # Simple dataloader with regression samples
        class RegressionDataLoader:
            def __init__(self):
                self.data = [
                    ([1], 2),    # Correct
                    ([2], 3),    # Off by 1
                    ([3], 6),    # Correct
                    ([4], 7),    # Off by 1
                    ([5], 10),   # Correct
                ]
    
            def __iter__(self):
                return iter(self.data)
    
        dataloader = RegressionDataLoader()
        output_dir = tmp_path / "regression_results"
    
        # Create experiment logger
        config = TelemetryConfig()
        logger = ExperimentLogger(config, {"model": "regression_model"})
    
        # Run evaluation
>       metrics = evaluate(
            model,
            dataloader,
            task_type="regression",
            experiment_logger=logger,
            output_dir=output_dir,
            epoch=1,
        )

tests/unit/test_evaluation_helpers.py:261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
granite-test-generator/src/eval/evaluate.py:79: in evaluate
    metrics = compute_regression_metrics(predictions, targets)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

predictions = [1, 1, 2, 2, 3, 3, ...], targets = [2, 3, 6, 7, 10]

    def compute_regression_metrics(predictions: Iterable, targets: Iterable) -> Mapping[str, float]:
        """Compute MAE, RMSE, and R^2 regression metrics."""
        preds = _flatten_float(predictions)
        gold = _flatten_float(targets)
        if len(preds) != len(gold):
>           raise ValueError("Predictions and targets must share the same length")
E           ValueError: Predictions and targets must share the same length

granite-test-generator/src/eval/metrics.py:94: ValueError
___________________ TestEvaluationHelpers.test_evaluate_text ___________________

self = <test_evaluation_helpers.TestEvaluationHelpers object at 0x3083bb330>
tmp_path = PosixPath('/private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_evaluate_text0')

    def test_evaluate_text(self, tmp_path: Path) -> None:
        """Test evaluate function with text generation task."""
        # Model that returns fixed responses
        responses = ["Hello world", "This is a test", "Machine learning"]
        iter_responses = iter(responses)
        model = lambda x: next(iter_responses)
    
        # Simple dataloader with text samples
        class TextDataLoader:
            def __init__(self):
                self.data = [
                    (["prompt1"], ["Hello world"]),          # Correct
                    (["prompt2"], ["Something different"]),  # Incorrect
                    (["prompt3"], ["Machine learning"]),     # Correct
                ]
    
            def __iter__(self):
                return iter(self.data)
    
        dataloader = TextDataLoader()
        output_dir = tmp_path / "text_results"
    
        # Create experiment logger
        config = TelemetryConfig()
        logger = ExperimentLogger(config, {"model": "text_model"})
    
        # Run evaluation
        metrics = evaluate(
            model,
            dataloader,
            task_type="text",
            experiment_logger=logger,
            output_dir=output_dir,
            epoch=1,
        )
    
        # Check metrics
>       logger.debug(f"Text metrics: {metrics}")
E       AttributeError: 'ExperimentLogger' object has no attribute 'debug'

tests/unit/test_evaluation_helpers.py:317: AttributeError
------------------------------ Captured log call -------------------------------
INFO     absl:rouge_scorer.py:83 Using default tokenizer.
INFO     src.eval.evaluate:evaluate.py:104 Saved evaluation report to /private/var/folders/1p/8_mzk6q95bv1w31pwcdlxwv80000gn/T/pytest-of-iancruickshank/pytest-116/test_evaluate_text0/text_results/eval_report.json
=============================== warnings summary ===============================
granite-test-generator/src/config/telemetry.py:65
  /Users/iancruickshank/GraniteMOE1B_SQE_Agent/granite-test-generator/src/config/telemetry.py:65: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/
    @validator("tb_log_dir")

granite-test-generator/src/config/telemetry.py:71
  /Users/iancruickshank/GraniteMOE1B_SQE_Agent/granite-test-generator/src/config/telemetry.py:71: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/
    @validator("wandb_tags", pre=True)

tests/unit/test_telemetry_config.py: 12 warnings
  /opt/anaconda3/lib/python3.13/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/
    warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)

tests/unit/test_experiment_logger.py::TestExperimentLogger::test_file_system_artifacts
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_metrics_with_different_types
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_git_sha_detection
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_git_sha_detection
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_artifact_with_nonexistent_path
tests/unit/test_experiment_logger.py::TestExperimentLogger::test_no_metrics
  /Users/iancruickshank/GraniteMOE1B_SQE_Agent/granite-test-generator/src/telemetry/experiment.py:230: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/unit/test_telemetry_config.py::TestTagSplitting::test_split_tags[123-expected7]
FAILED tests/unit/test_telemetry_config.py::TestLoadTelemetryFromSources::test_invalid_log_interval_steps
FAILED tests/unit/test_experiment_logger.py::TestExperimentLogger::test_file_system_artifacts
FAILED tests/unit/test_experiment_logger.py::TestExperimentLogger::test_run_name_generation
FAILED tests/unit/test_experiment_logger.py::TestExperimentLogger::test_experiment_logger_with_real_directory_structure
FAILED tests/unit/test_metrics.py::TestClassificationMetrics::test_binary_classification
FAILED tests/unit/test_metrics.py::TestClassificationMetrics::test_multiclass_classification
FAILED tests/unit/test_metrics.py::TestRegressionMetrics::test_basic_regression
FAILED tests/unit/test_metrics.py::TestRegressionMetrics::test_perfect_predictions
FAILED tests/unit/test_metrics.py::TestRegressionMetrics::test_empty_predictions_regression
FAILED tests/unit/test_metrics.py::TestTextGenerationMetrics::test_basic_text_metrics
FAILED tests/unit/test_metrics.py::TestTextGenerationMetrics::test_empty_text_predictions
FAILED tests/unit/test_metrics.py::TestTextGenerationMetrics::test_mismatched_lengths_text
FAILED tests/unit/test_metrics.py::TestTextGenerationMetrics::test_invalid_references_format
FAILED tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_flatten_probabilities_torch
FAILED tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_evaluate_classification
FAILED tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_evaluate_regression
FAILED tests/unit/test_evaluation_helpers.py::TestEvaluationHelpers::test_evaluate_text
================= 18 failed, 67 passed, 20 warnings in 13.55s ==================
